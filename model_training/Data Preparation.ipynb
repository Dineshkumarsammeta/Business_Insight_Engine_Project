# Step 1: Import pandas library for data manipulation
import pandas as pd

# Step 2: Load the Amazon Fine Food Reviews dataset into a pandas DataFrame
df = pd.read_csv("/kaggle/input/amazon-fine-food-reviews/Reviews.csv")

# Step 3: Take a peek at the dataset to see how it looks
df

# Step 4: Check basic info about the dataset like number of rows, columns, and data types
df.info()

# Step 5: Get summary statistics for numerical columns to understand the data distribution
df.describe()

# Step 6: Check for missing values in each column
df.isnull().sum()

# Step 7: Check how many reviews exist for each score (1-5)
df["Score"].value_counts()

# Step 8: View the first review text
df["Text"][0]

# Step 9: View the first review summary
df["Summary"][0]

# Step 10: Remove rows with missing values
df = df.dropna()

# Step 11: Reset the DataFrame index after dropping rows
df.reset_index(inplace=True)

# Step 12: Drop the old 'index' column created during reset_index
df = df.drop(["index"], axis=1)

# Step 13: Check for duplicate rows in the dataset
duplicate_rows = df[df.duplicated()]
print('Number of duplicate tweets : ', len(duplicate_rows))

# Step 14: Text Preprocessing - Install BeautifulSoup for HTML parsing
!pip install bs4

# Step 15: Import NLTK for natural language processing and download stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# Step 16: Function to expand contractions in text (e.g., "won't" -> "will not")
import re

def decontracted(phrase):
    # Handle specific contractions first
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)
    
    # Handle general contractions
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

# Step 17: Preprocessing function for reviews
from bs4 import BeautifulSoup

def preprocessed(review):
    # Remove URLs
    sentance = re.sub(r"http\S+", "", review)
    # Remove HTML tags
    sentance = BeautifulSoup(sentance, 'lxml').get_text()
    # Expand contractions
    sentance = decontracted(sentance)
    # Remove words with numbers
    sentance = re.sub("\S*\d\S*", "", sentance).strip()
    # Keep only letters and remove everything else
    sentance = re.sub('[^A-Za-z]+', ' ', sentance)
    # Remove stopwords and convert to lowercase
    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords.words('english'))
    return sentance.strip()

# Step 18: Test preprocessing on a sample review
print("Review : ", df["Text"][0], "\n")
print("Preprocessed Review : ", preprocessed(df["Text"][0]))

# Step 19: Test preprocessing on a sample summary
print("Summary : ", df["Summary"][0], "\n")
print("Preprocessed Summary : ", preprocessed(df["Summary"][0]))

# Step 20: Enable progress bars for pandas operations
from tqdm.notebook import tqdm
tqdm.pandas()

# Step 21: Sample 20,000 reviews for faster processing
df = df.sample(20000)
df.reset_index(inplace=True)

# Step 22: Apply the preprocessing function to all reviews
df["Preprocessed Review"] = df["Text"].progress_apply(lambda x: preprocessed(x))

# Step 23: Calculate the length of each review in terms of word count
from tqdm import tqdm
l = []
for x in tqdm(df["Preprocessed Review"]):
    l.append(len(x.split()))

# Step 24: Add the review length as a new column
df["Review_length"] = l

# Step 25: Check the average review length
df["Review_length"].mean()

# Step 26: Check the distribution of review scores after sampling
df["Score"].value_counts()
